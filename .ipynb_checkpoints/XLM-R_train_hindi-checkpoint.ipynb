{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLM-Roberta Sentiment Analysis\n",
    "## Hindi-English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.5 (default, Sep 18 2020, 23:02:24) \\n[Clang 11.0.3 (clang-1103.0.32.62)]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-57cdb37d0bf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# If there's a GPU available...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "# import our packages...\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parsing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences in training set: 13,935\n",
      "\n",
      "Number of training sentences in val set: 2,988\n",
      "\n",
      "Train dataset: \n",
      "       id                                           sentence  label sentiment\n",
      "0   4330  nen á vist bolest vztek smutek zmatek osam ě l...      1   neutral\n",
      "1  41616  Haan yaar neha pensive pensive kab karega woh ...      1   neutral\n",
      "2   6648  television media congress ke liye nhi h Ye toh...      0  negative\n",
      "3   2512  All India me nrc lagu kare w Kashmir se dhara ...      2  positive\n",
      "4    610  who Pagal hai kya They aren t real issues Mand...      1   neutral\n",
      "Val dataset: \n",
      "       id                                           sentence  label sentiment\n",
      "0  30258  modi mantrimandal may samil honay par badhai n...      2  positive\n",
      "1  16648                Rashid Tu toh naamakool hai Mare h       0  negative\n",
      "2  28511  U saw caste and religion in them nation saw ta...      0  negative\n",
      "3  10466  sir local police station pe complaint krne par...      1   neutral\n",
      "4  19266  Ve Maahi song from Kesari is current favourite...      2  positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets into a pandas dataframe.\n",
    "df_train = pd.read_csv(\"data/hindi-english/train_14k_split.csv\" )\n",
    "df_val = pd.read_csv(\"data/hindi-english/val_3k_split.csv\" )\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences in training set: {:,}\\n'.format(df_train.shape[0]))\n",
    "print('Number of training sentences in val set: {:,}\\n'.format(df_val.shape[0]))\n",
    "\n",
    "# Get the lists of sentences and their labels for train and val datasets\n",
    "sentences_train = df_train.sentence.values\n",
    "labels_train = df_train.label.values\n",
    "sentences_val = df_val.sentence.values\n",
    "labels_val = df_val.label.values\n",
    "\n",
    "print('Train dataset: \\n', df_train.head())\n",
    "print('Val dataset: \\n',df_val.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading XLMRobertaTokenizer ...\n",
      "Original Train Sentence:  ONE OF MY FAVORITE PHOTO BY THE WAY SHAM USE IT AS A WHATS APP WALLPAPER zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye zany face grinning face with one large and one small eye \n",
      "Tokenized Train Sentence:  ['▁nen', '▁á', '▁vist', '▁bolest', '▁vz', 'tek', '▁smut', 'ek', '▁z', 'ma', 'tek', '▁osam', '▁', 'ě', '▁lost', '▁bez', 'nad', '▁', 'ě', '▁j', '▁a', '▁nakonec', '▁jen', '▁klid', '▁Asi', '▁takhle', '▁vy', 'pad', '▁á', '▁m', '▁', 'ů', '▁j', '▁life']\n",
      "Token Train IDs:  [73351, 392, 18591, 112616, 10682, 2142, 67456, 343, 97, 192, 2142, 99892, 6, 2353, 72856, 1209, 9169, 6, 2353, 1647, 10, 95002, 7349, 122166, 37933, 167809, 1154, 4299, 392, 347, 6, 1170, 1647, 6897]\n",
      "Original Val Sentence:  modi mantrimandal may samil honay par badhai narmaday har\n",
      "Tokenized Train Sentence:  ['▁modi', '▁man', 'tri', 'man', 'dal', '▁may', '▁sam', 'il', '▁ho', 'nay', '▁par', '▁bad', 'hai', '▁na', 'rma', 'day', '▁har']\n",
      "Token Train IDs:  [43381, 332, 3996, 669, 2465, 1543, 1289, 379, 739, 13650, 366, 6494, 15251, 24, 17668, 5636, 182]\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading XLMRobertaTokenizer ...')\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', do_lower_case=True)\n",
    "tokenized_train = tokenizer.tokenize(sentences_train[0])\n",
    "tokenized_ids_train = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences_train[0]))\n",
    "tokenized_val = tokenizer.tokenize(sentences_val[0])\n",
    "tokenized_ids_val = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences_val[0]))\n",
    "\n",
    "# Print the original train sentence; tokenized and IDs mapped.\n",
    "print('Original Train Sentence: ', sentences_train[13249])\n",
    "print('Tokenized Train Sentence: ', tokenized_train)\n",
    "print('Token Train IDs: ', tokenized_ids_train)\n",
    "\n",
    "# Print the original val sentence; tokenized and IDs mapped.\n",
    "print('Original Val Sentence: ', sentences_val[0])\n",
    "print('Tokenized Train Sentence: ', tokenized_val)\n",
    "print('Token Train IDs: ', tokenized_ids_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Got 256 and 289 in dimension 1 (The offending index is 13249)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-0ea90aa05d4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Convert the lists into tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0minput_ids_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mattention_masks_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_masks_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mlabels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Got 256 and 289 in dimension 1 (The offending index is 13249)"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_train = []\n",
    "attention_masks_train = []\n",
    "input_ids_val = []\n",
    "attention_masks_val = []\n",
    "\n",
    "# Loop through sentences for Train and Val datasets\n",
    "for sent in sentences_train:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids_train.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # Add its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks_train.append(encoded_dict['attention_mask'])\n",
    "\n",
    "for sent in sentences_val:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 256,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )\n",
    "\n",
    "    input_ids_val.append(encoded_dict['input_ids'])\n",
    "    attention_masks_val.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "# Convert the lists into tensors.\n",
    "input_ids_train = torch.cat(input_ids_train, dim=0)\n",
    "attention_masks_train = torch.cat(attention_masks_train, dim=0)\n",
    "labels_train = torch.tensor(labels_train)\n",
    "\n",
    "input_ids_val = torch.cat(input_ids_val, dim=0)\n",
    "attention_masks_val = torch.cat(attention_masks_val, dim=0)\n",
    "labels_val = torch.tensor(labels_val)\n",
    "\n",
    "print('Original train sentence \\n: ', sentences_train[1])\n",
    "print('Train Token IDs \\n:', input_ids_train[1])\n",
    "print('Train labels: \\n', labels_train)\n",
    "print('\\n')\n",
    "print('Original val sentence \\n: ', sentences_train[1])\n",
    "print('Val Token IDs \\n:', input_ids_train[1])\n",
    "print('Val labels: \\n', labels_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Train and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "val_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Train Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
